{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e7566e-a5d1-4d4b-960a-0cbac91f910e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5741184051847129>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTF_ENABLE_ONEDNN_OPTS\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
       "\n",
       "File \u001B[0;32m<frozen os>:690\u001B[0m, in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n",
       "\n",
       "File \u001B[0;32m<frozen os>:764\u001B[0m, in \u001B[0;36mencode\u001B[0;34m(value)\u001B[0m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: str expected, not int"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "str expected, not int"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5741184051847129>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTF_ENABLE_ONEDNN_OPTS\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
        "File \u001B[0;32m<frozen os>:690\u001B[0m, in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n",
        "File \u001B[0;32m<frozen os>:764\u001B[0m, in \u001B[0;36mencode\u001B[0;34m(value)\u001B[0m\n",
        "\u001B[0;31mTypeError\u001B[0m: str expected, not int"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86edeb7-8916-4c5d-957c-11d4c90c7c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#1 - Gera√ß√£o de Texto com Modelo Causal (Tucano-2b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e53e7d-41a5-42fe-acd0-5d6c1bbb54fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio demonstra como gerar texto com m√∫ltiplos prompts utilizando um modelo do tipo Causal Language Model (CLM), com execu√ß√£o paralela na GPU. Mostra como medir tempo de infer√™ncia e como o batch size impacta na performance.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Gera√ß√£o de texto autom√°tico em portugu√™s, a partir de instru√ß√µes ou in√≠cios de frase (prompts). √ötil para gera√ß√£o criativa, completamento de respostas, storytelling e chatbots.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "Input: Lista de frases iniciais (prompts) em portugu√™s.\n",
    "Output: Textos gerados pelo modelo, um para cada prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d970843-a532-488f-9c85-5e3c1e614eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d451389e73e4985992805c795c418d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGera√ß√£o conclu√≠da em 3.73 segundos\n\n---- Sa√≠da 1 ----\nA XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) acontecer√° em Foz do Igua√ßu. Durante o evento, os especialistas discutir√£o a rela√ß√£o entre a Ci√™ncia da Computa√ß√£o, as Tecnologias da Informa√ß√£o e Comunica√ß√£o (TICs) e o Desenvolvimento Sustent√°vel.\nProfessores e alunos das √°reas de Ci√™ncias Exatas, Engenharias e Tecnologia, bem como representantes de institui√ß√µes p√∫blicas e privadas, est√£o convidados para participar do encontro. Para participar, inscreva-se no\n\n---- Sa√≠da 2 ----\nOs principais objetivos da ERAD/RS incluem qualificar profissionais em processamento de alto desempenho. No futuro, espera-se que a escola seja um centro para estudantes e docentes de p√≥s-gradua√ß√£o na √°rea de processamento digital de sinais e processamento paralelo. Al√©m disso, espera-se, tamb√©m, uma forte colabora√ß√£o com o Instituto Tecnol√≥gico e de Pesquisas do Estado do Rio Grande do Sul (ITP/RS) e a Universidade Federal do Pampa (Unipampa), entre outros\n\n---- Sa√≠da 3 ----\nEntre 23 e 25 de abril de 2025, participantes visitar√£o o Itaipu Parquetec, onde ter√£o a oportunidade de conhecer projetos de inova√ß√£o em diversos setores, como sustentabilidade, energia renov√°vel, biotecnologia e agricultura digital. O evento √© uma iniciativa do Parque Tecnol√≥gico Itaipaiva (TecnoParq), unidade de neg√≥cios da Funda√ß√£o Parque Cient√≠fico e Tecnol√≥gico, em parceria com a ItaIPU e a\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001B[0m, in \u001B[0;36mcached_files\u001B[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n",
       "\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m    423\u001B[0m     \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n",
       "\u001B[0;32m--> 424\u001B[0m     hf_hub_download(\n",
       "\u001B[1;32m    425\u001B[0m         path_or_repo_id,\n",
       "\u001B[1;32m    426\u001B[0m         filenames[\u001B[38;5;241m0\u001B[39m],\n",
       "\u001B[1;32m    427\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n",
       "\u001B[1;32m    428\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n",
       "\u001B[1;32m    429\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n",
       "\u001B[1;32m    430\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n",
       "\u001B[1;32m    431\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n",
       "\u001B[1;32m    432\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n",
       "\u001B[1;32m    433\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n",
       "\u001B[1;32m    434\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n",
       "\u001B[1;32m    435\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n",
       "\u001B[1;32m    436\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n",
       "\u001B[1;32m    437\u001B[0m     )\n",
       "\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m--> 106\u001B[0m     validate_repo_id(arg_value)\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n",
       "\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
       "\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n",
       "\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must use alphanumeric chars or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    162\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m forbidden, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot start or end the name, max length is 96:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    164\u001B[0m     )\n",
       "\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id:\n",
       "\n",
       "\u001B[0;31mHFValidationError\u001B[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'Tucano-2b4-Instruct\t'.\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5741184051847164>, line 12\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m SEED        \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m42\u001B[39m\n",
       "\u001B[1;32m     10\u001B[0m random\u001B[38;5;241m.\u001B[39mseed(SEED); torch\u001B[38;5;241m.\u001B[39mmanual_seed(SEED)\n",
       "\u001B[0;32m---> 12\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(MODEL_ID, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n",
       "\u001B[1;32m     13\u001B[0m model     \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(MODEL_ID, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n",
       "\u001B[1;32m     14\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:910\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    907\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    909\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n",
       "\u001B[0;32m--> 910\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m get_tokenizer_config(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    911\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m tokenizer_config:\n",
       "\u001B[1;32m    912\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:742\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    739\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n",
       "\u001B[1;32m    741\u001B[0m commit_hash \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[0;32m--> 742\u001B[0m resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n",
       "\u001B[1;32m    743\u001B[0m     pretrained_model_name_or_path,\n",
       "\u001B[1;32m    744\u001B[0m     TOKENIZER_CONFIG_FILE,\n",
       "\u001B[1;32m    745\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n",
       "\u001B[1;32m    746\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n",
       "\u001B[1;32m    747\u001B[0m     resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n",
       "\u001B[1;32m    748\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n",
       "\u001B[1;32m    749\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n",
       "\u001B[1;32m    750\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n",
       "\u001B[1;32m    751\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n",
       "\u001B[1;32m    752\u001B[0m     subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n",
       "\u001B[1;32m    753\u001B[0m     _raise_exceptions_for_gated_repo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    754\u001B[0m     _raise_exceptions_for_missing_entries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    755\u001B[0m     _raise_exceptions_for_connection_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    756\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n",
       "\u001B[1;32m    757\u001B[0m )\n",
       "\u001B[1;32m    758\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    759\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcached_file\u001B[39m(\n",
       "\u001B[1;32m    209\u001B[0m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n",
       "\u001B[1;32m    210\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n",
       "\u001B[1;32m    211\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m    212\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mstr\u001B[39m]:\n",
       "\u001B[1;32m    213\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    214\u001B[0m \u001B[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n",
       "\u001B[1;32m    215\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n",
       "\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 266\u001B[0m     file \u001B[38;5;241m=\u001B[39m cached_files(path_or_repo_id\u001B[38;5;241m=\u001B[39mpath_or_repo_id, filenames\u001B[38;5;241m=\u001B[39m[filename], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    267\u001B[0m     file \u001B[38;5;241m=\u001B[39m file[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n",
       "\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:471\u001B[0m, in \u001B[0;36mcached_files\u001B[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n",
       "\u001B[1;32m    463\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n",
       "\u001B[1;32m    464\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    465\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    466\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    467\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m    469\u001B[0m \u001B[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001B[39;00m\n",
       "\u001B[1;32m    470\u001B[0m resolved_files \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[0;32m--> 471\u001B[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m full_filenames\n",
       "\u001B[1;32m    472\u001B[0m ]\n",
       "\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m resolved_files):\n",
       "\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resolved_files\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:134\u001B[0m, in \u001B[0;36m_get_cache_file_to_return\u001B[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001B[0m\n",
       "\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_cache_file_to_return\u001B[39m(\n",
       "\u001B[1;32m    131\u001B[0m     path_or_repo_id: \u001B[38;5;28mstr\u001B[39m, full_filename: \u001B[38;5;28mstr\u001B[39m, cache_dir: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, revision: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m ):\n",
       "\u001B[1;32m    133\u001B[0m     \u001B[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001B[39;00m\n",
       "\u001B[0;32m--> 134\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir, revision\u001B[38;5;241m=\u001B[39mrevision)\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m resolved_file \u001B[38;5;241m!=\u001B[39m _CACHED_NO_EXIST:\n",
       "\u001B[1;32m    136\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m arg_name, arg_value \u001B[38;5;129;01min\u001B[39;00m chain(\n",
       "\u001B[1;32m    102\u001B[0m     \u001B[38;5;28mzip\u001B[39m(signature\u001B[38;5;241m.\u001B[39mparameters, args),  \u001B[38;5;66;03m# Args values\u001B[39;00m\n",
       "\u001B[1;32m    103\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mitems(),  \u001B[38;5;66;03m# Kwargs values\u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m ):\n",
       "\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\u001B[0;32m--> 106\u001B[0m         validate_repo_id(arg_value)\n",
       "\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    109\u001B[0m         has_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n",
       "\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n",
       "\u001B[1;32m    155\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    157\u001B[0m     )\n",
       "\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n",
       "\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n",
       "\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must use alphanumeric chars or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    162\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m forbidden, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot start or end the name, max length is 96:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    164\u001B[0m     )\n",
       "\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id:\n",
       "\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot have -- or .. in repo_id: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mHFValidationError\u001B[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'Tucano-2b4-Instruct\t'."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "HFValidationError",
        "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'Tucano-2b4-Instruct\t'."
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001B[0m, in \u001B[0;36mcached_files\u001B[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[0;32m--> 424\u001B[0m     hf_hub_download(\n\u001B[1;32m    425\u001B[0m         path_or_repo_id,\n\u001B[1;32m    426\u001B[0m         filenames[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    427\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[1;32m    428\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    429\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    430\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    431\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    432\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    433\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    434\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    435\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    436\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    437\u001B[0m     )\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 106\u001B[0m     validate_repo_id(arg_value)\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must use alphanumeric chars or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m forbidden, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot start or end the name, max length is 96:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    164\u001B[0m     )\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id:\n",
        "\u001B[0;31mHFValidationError\u001B[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'Tucano-2b4-Instruct\t'.",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5741184051847164>, line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m SEED        \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     10\u001B[0m random\u001B[38;5;241m.\u001B[39mseed(SEED); torch\u001B[38;5;241m.\u001B[39mmanual_seed(SEED)\n\u001B[0;32m---> 12\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(MODEL_ID, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n\u001B[1;32m     13\u001B[0m model     \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(MODEL_ID, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n\u001B[1;32m     14\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:910\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    907\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    909\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n\u001B[0;32m--> 910\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m get_tokenizer_config(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    911\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m tokenizer_config:\n\u001B[1;32m    912\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:742\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n\u001B[1;32m    739\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n\u001B[1;32m    741\u001B[0m commit_hash \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 742\u001B[0m resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[1;32m    743\u001B[0m     pretrained_model_name_or_path,\n\u001B[1;32m    744\u001B[0m     TOKENIZER_CONFIG_FILE,\n\u001B[1;32m    745\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    746\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    747\u001B[0m     resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    748\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    749\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    750\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    751\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    752\u001B[0m     subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m    753\u001B[0m     _raise_exceptions_for_gated_repo\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    754\u001B[0m     _raise_exceptions_for_missing_entries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    755\u001B[0m     _raise_exceptions_for_connection_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    756\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m    757\u001B[0m )\n\u001B[1;32m    758\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    759\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, **kwargs)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcached_file\u001B[39m(\n\u001B[1;32m    209\u001B[0m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[1;32m    210\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    212\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    213\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[1;32m    215\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m     file \u001B[38;5;241m=\u001B[39m cached_files(path_or_repo_id\u001B[38;5;241m=\u001B[39mpath_or_repo_id, filenames\u001B[38;5;241m=\u001B[39m[filename], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    267\u001B[0m     file \u001B[38;5;241m=\u001B[39m file[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:471\u001B[0m, in \u001B[0;36mcached_files\u001B[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    464\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    465\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    466\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    467\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001B[39;00m\n\u001B[1;32m    470\u001B[0m resolved_files \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 471\u001B[0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m full_filenames\n\u001B[1;32m    472\u001B[0m ]\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m resolved_files):\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resolved_files\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/utils/hub.py:134\u001B[0m, in \u001B[0;36m_get_cache_file_to_return\u001B[0;34m(path_or_repo_id, full_filename, cache_dir, revision)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_cache_file_to_return\u001B[39m(\n\u001B[1;32m    131\u001B[0m     path_or_repo_id: \u001B[38;5;28mstr\u001B[39m, full_filename: \u001B[38;5;28mstr\u001B[39m, cache_dir: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, revision: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    132\u001B[0m ):\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001B[38;5;241m=\u001B[39mcache_dir, revision\u001B[38;5;241m=\u001B[39mrevision)\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m resolved_file \u001B[38;5;241m!=\u001B[39m _CACHED_NO_EXIST:\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m arg_name, arg_value \u001B[38;5;129;01min\u001B[39;00m chain(\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28mzip\u001B[39m(signature\u001B[38;5;241m.\u001B[39mparameters, args),  \u001B[38;5;66;03m# Args values\u001B[39;00m\n\u001B[1;32m    103\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mitems(),  \u001B[38;5;66;03m# Kwargs values\u001B[39;00m\n\u001B[1;32m    104\u001B[0m ):\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 106\u001B[0m         validate_repo_id(arg_value)\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    109\u001B[0m         has_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    155\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must be in the form \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnamespace/repo_name\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Use `repo_type` argument if needed.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    157\u001B[0m     )\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must use alphanumeric chars or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m forbidden, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot start or end the name, max length is 96:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    164\u001B[0m     )\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id:\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot have -- or .. in repo_id: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mHFValidationError\u001B[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'Tucano-2b4-Instruct\t'."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ========================\n",
    "# Configura√ß√µes iniciais\n",
    "# ========================\n",
    "# TODO: voc√™ pode alterar o modelo aqui para outro compat√≠vel do Hugging Face\n",
    "MODEL_ID    = \"TucanoBR/Tucano-2b4-Instruct\"\n",
    "CACHE_DIR   = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW     = 64                  # n√∫mero de tokens a serem gerados\n",
    "TEMPERATURE = 0.7                 # controla a aleatoriedade da gera√ß√£o\n",
    "SEED        = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========================\n",
    "# Carregamento do modelo\n",
    "# ========================\n",
    "# Usamos cache local para acelerar a reutiliza√ß√£o\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # evita warning no Hugging Face\n",
    "\n",
    "# ========================\n",
    "# Prompts de entrada\n",
    "# ========================\n",
    "# TODO: adicione outros prompts aqui para testar varia√ß√µes de sa√≠da\n",
    "prompts = [\n",
    "    \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) acontecer√° em Foz do Igua√ßu. Durante o evento, os especialistas discutir√£o\",\n",
    "    \"Os principais objetivos da ERAD/RS incluem qualificar profissionais em processamento de alto desempenho. No futuro, espera-se que\",\n",
    "    \"Entre 23 e 25 de abril de 2025, participantes visitar√£o o Itaipu Parquetec, onde\"\n",
    "]\n",
    "\n",
    "# ====================================================\n",
    "# Fun√ß√£o de gera√ß√£o de texto em lote com GPU\n",
    "# ====================================================\n",
    "@torch.inference_mode()  # Desativa o autograd para infer√™ncia mais leve\n",
    "def generate_batch(texts):\n",
    "    # Verifica se h√° GPU dispon√≠vel\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"GPU requerida mas n√£o encontrada.\")\n",
    "\n",
    "    # Move o modelo para GPU ‚Äî ativando paralelismo nos kernels da gera√ß√£o\n",
    "    model.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "    # Tokeniza√ß√£o em lote (paralelismo ativado aqui)\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,              # garante alinhamento do batch\n",
    "        padding_side='left'        # importante para modelos auto-regressivos\n",
    "    ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "    # Sincroniza a GPU antes de medir tempo\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Gera√ß√£o paralela em lote (CUDA executa m√∫ltiplas sequ√™ncias simultaneamente)\n",
    "    outs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW,\n",
    "        do_sample=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.15,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Espera final da GPU antes de medir tempo total\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    print(f\"\\nGera√ß√£o conclu√≠da em {elapsed:.2f} segundos\\n\")\n",
    "\n",
    "    # Decodifica√ß√£o e impress√£o\n",
    "    for i, txt in enumerate(tokenizer.batch_decode(outs, skip_special_tokens=True), 1):\n",
    "        print(f\"---- Sa√≠da {i} ----\\n{txt}\\n\")\n",
    "\n",
    "# ========================\n",
    "# Execu√ß√£o principal\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: experimente rodar esta fun√ß√£o com mais ou menos prompts\n",
    "    generate_batch(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e687a9-8efe-4a1d-8117-77d797f50e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 10:35:59.278525: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-25 10:35:59.294746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-25 10:35:59.314821: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-25 10:35:59.320850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-25 10:35:59.335098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-25 10:36:03.324148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadf286272b84d6c80a5578e840d440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a4f291444e4277bbc6963d903f4d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393fe32b24b746bcbd92f119e85c79e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54e2b310e04416f8f6ea2d74f4e011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9208cc2a270e4ad6ac6e8f6294c8e4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02820d2c1680425292ba303c71953f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fba592cdde34616bcd64e70ea496419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354456c64a664aa7a981089923364c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad03a39c43da4a8497a576cf93a6426c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4622823a27e744a18091c34e70c59d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    N | CPU-SEQ (s) |  GPU (s) | Speed-up\n-------------------------------------------\n    1 |        7.73 |     3.65 |   2.12√ó\n    2 |       15.17 |     3.59 |   4.23√ó\n    4 |       30.34 |     3.82 |   7.95√ó\n    8 |       57.74 |     4.57 |  12.64√ó\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict\n",
    "\n",
    "# --------------------------- Configura√ß√£o do experimento ---------------------------\n",
    "MODEL_ID   = \"TucanoBR/Tucano-2b4-Instruct\"    # Modelo causal da Hugging Face\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW    = 64                                # tokens a serem gerados\n",
    "TEMPERATURE = 0.7                              # aleatoriedade da gera√ß√£o\n",
    "SIZES      = [2**i for i in range(7)]          # tamanhos de lote: 1, 2, 4, 8, 16, 32, 64\n",
    "SEED       = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Fun√ß√£o para carregar o modelo e tokenizer\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # define token de padding necess√°rio para batching\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Fun√ß√£o que gera *n* prompts distintos\n",
    "# ----------------------------------------------------------------------------\n",
    "def make_prompts(n: int) -> List[str]:\n",
    "    base = \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) acontecer√° em Foz do Igua√ßu.\"\n",
    "    return [f\"{base} ‚Äî example {i}\" for i in range(n)]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Execu√ß√£o sequencial na CPU (sem paralelismo entre prompts)\n",
    "# ----------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(prompts: List[str]) -> float:\n",
    "    \"\"\"Gera texto um por um na CPU. Medimos o tempo total.\"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for p in prompts:\n",
    "        inputs = tokenizer(p, return_tensors=\"pt\", padding=True).to(\"cpu\")\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            do_sample=True, temperature=TEMPERATURE,\n",
    "            top_k=50, top_p=0.9,\n",
    "            repetition_penalty=1.15,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Execu√ß√£o em lote na GPU (ativando paralelismo verdadeiro)\n",
    "# ----------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(prompts: List[str]) -> float | None:\n",
    "    \"\"\"Gera todos os prompts em batch na GPU. Retorna o tempo total.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        # Aqui ativamos o paralelismo de entrada (tokeniza√ß√£o em lote)\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # garante in√≠cio de medi√ß√£o ap√≥s aloca√ß√£o\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        # O paralelismo √© totalmente explorado aqui:\n",
    "        outs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            do_sample=True, temperature=TEMPERATURE,\n",
    "            top_k=50, top_p=0.9,\n",
    "            repetition_penalty=1.15,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()  # espera conclus√£o total da gera√ß√£o\n",
    "        return time.perf_counter() - t0\n",
    "\n",
    "    finally:\n",
    "        # Limpa mem√≥ria GPU ap√≥s a execu√ß√£o\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Fun√ß√£o principal de benchmark: compara CPU sequencial vs. GPU em lote\n",
    "# ----------------------------------------------------------------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        prompts = make_prompts(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(prompts)\n",
    "        t_gpu = run_gpu_batch(prompts)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Execu√ß√£o principal: inicializa o benchmark e imprime a tabela comparativa\n",
    "# ----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec012f49-648c-4b17-96fe-830a873c6b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#2 - Infer√™ncia para An√°lise de Sentimentos em Portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b3ae86-f956-43ea-9cd3-28850108b388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio demonstra como realizar infer√™ncia paralela (batch) em GPU utilizando um modelo pr√©-treinado em portugu√™s (pysentimiento/bertweet-pt-sentiment) para classifica√ß√£o bin√°ria de sentimento (positivo ou negativo). A atividade mostra como medir o tempo de execu√ß√£o e comparar os ganhos de performance obtidos com uso de paralelismo na GPU, em contraste com execu√ß√£o sequencial na CPU.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Classifica√ß√£o autom√°tica do sentimento em textos curtos, como coment√°rios, avalia√ß√µes ou tweets. √ötil para an√°lise de redes sociais, pesquisas de opini√£o, monitoramento de eventos e outras aplica√ß√µes de NLP em portugu√™s.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "Input: Lista de frases em portugu√™s, curtas e de opini√£o.\n",
    "Output: Para cada frase, o modelo retorna:\n",
    "- O r√≥tulo de sentimento (positivo ou negativo)\n",
    "- O score de confian√ßa (probabilidade associada √† predi√ß√£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce7a206-e3a0-4808-a60e-3d2b9fd2c4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e8f9cd5584f1bbcbecb9e99116e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4a7fda818e4aada6e66976066e99a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/799k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b762b867e9cf4635afc22046f710c3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14d693bcd5a48b4ae8875df1a82298d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0b9a42cb594adfb78baf024ae4fff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be3dc803bd4b85a304de2692cb8b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/952 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c06c1185f94e4495ea6d3b0aadd664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nInfer√™ncia em GPU conclu√≠da em 0.07 segundos.\n\nFrase: Estou muito animado para participar da XXV ERAD/RS 2025 em Foz do Igua√ßu!\nSentimento: positivo (confian√ßa 0.95)\n\nFrase: Infelizmente perdi o prazo de submiss√£o de trabalhos para a ERAD, fiquei desapontado.\nSentimento: negativo (confian√ßa 0.99)\n\nFrase: O cronograma de minicursos sobre computa√ß√£o exaescala parece excelente.\nSentimento: positivo (confian√ßa 0.98)\n\nFrase: A prorroga√ß√£o das inscri√ß√µes de novo foi frustrante para alguns participantes.\nSentimento: negativo (confian√ßa 0.95)\n\nFrase: O F√≥rum de Inicia√ß√£o Cient√≠fica da ERAD/RS costuma ser inspirador e motivador.\nSentimento: positivo (confian√ßa 0.95)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ÉO DO MODELO E DADOS\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID  = \"pysentimiento/bertweet-pt-sentiment\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "\n",
    "# TODO: voc√™ pode alterar, remover ou adicionar frases de teste aqui\n",
    "SENTENCES = [\n",
    "    \"Estou muito animado para participar da XXV ERAD/RS 2025 em Foz do Igua√ßu!\",\n",
    "    \"Infelizmente perdi o prazo de submiss√£o de trabalhos para a ERAD, fiquei desapontado.\",\n",
    "    \"O cronograma de minicursos sobre computa√ß√£o exaescala parece excelente.\",\n",
    "    \"A prorroga√ß√£o das inscri√ß√µes de novo foi frustrante para alguns participantes.\",\n",
    "    \"O F√≥rum de Inicia√ß√£o Cient√≠fica da ERAD/RS costuma ser inspirador e motivador.\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO PARA CARREGAR O PIPELINE DE SENTIMENTO\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_pipeline(model_id: str, cache_dir: str) -> TextClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Carrega tokenizer e modelo diretamente para a GPU (caso dispon√≠vel).\n",
    "    A acelera√ß√£o √© ativada via `device=0`, indicando uso de CUDA.\n",
    "    \"\"\"\n",
    "    device = 0 if torch.cuda.is_available() else -1  # -1 = CPU\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "\n",
    "    # O uso de float16 reduz uso de mem√≥ria e acelera opera√ß√µes em GPU Ampere+\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else None\n",
    "    )\n",
    "\n",
    "    return TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO DE INFER√äNCIA COM MEDI√á√ÉO DE TEMPO\n",
    "# ------------------------------------------------------------------------------\n",
    "def run_gpu_batch(sentences: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Realiza infer√™ncia paralela na GPU com todas as frases de uma vez (batch_size=n).\n",
    "    Mede tempo total e imprime r√≥tulos e scores.\n",
    "    \"\"\"\n",
    "    pipe = load_pipeline(MODEL_ID, CACHE_DIR)\n",
    "\n",
    "    torch.cuda.synchronize()  # garante que tudo esteja ocioso antes da medi√ß√£o\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Aqui ocorre a infer√™ncia paralela real ‚Äî todas as frases s√£o processadas\n",
    "    # em paralelo no Tensor Core da GPU, com otimiza√ß√£o por lote\n",
    "    outputs = pipe(sentences, batch_size=len(sentences), truncation=True)\n",
    "\n",
    "    torch.cuda.synchronize()  # espera todos os kernels CUDA terminarem\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    print(f\"\\nInfer√™ncia em GPU conclu√≠da em {elapsed:.2f} segundos.\\n\")\n",
    "\n",
    "    # Impress√£o dos resultados\n",
    "    for s, out in zip(sentences, outputs):\n",
    "        lbl = \"positivo\" if out[\"label\"] == \"POS\" else \"negativo\"\n",
    "        print(f\"Frase: {s}\\nSentimento: {lbl} (confian√ßa {out['score']:.2f})\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_gpu_batch(SENTENCES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4694c352-2511-4536-97a7-f57e28a5adaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    N | CPU-SEQ (s) |  GPU (s) | Speed-up\n-------------------------------------------\n    1 |        0.03 |     0.01 |   3.97√ó\n    2 |        0.05 |     0.01 |   5.10√ó\n    4 |        0.11 |     0.01 |   8.70√ó\n    8 |        0.19 |     0.02 |  10.58√ó\n   16 |        0.29 |     0.03 |   9.37√ó\n   32 |        0.58 |     0.03 |  17.10√ó\n   64 |        1.14 |     0.06 |  20.60√ó\n  128 |        2.39 |     0.20 |  11.80√ó\n  256 |        4.75 |     0.31 |  15.47√ó\n  512 |        9.23 |     0.63 |  14.62√ó\n 1024 |       18.80 |     1.26 |  14.89√ó\n 2048 |       38.63 |     2.33 |  16.59√ó\n 4096 |       75.33 |     4.79 |  15.72√ó\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Dict\n",
    "\n",
    "# --------------------------- Configura√ß√µes Globais ---------------------------\n",
    "MODEL_ID  = \"pysentimiento/bertweet-pt-sentiment\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "SIZES     = [2**i for i in range(13)]\n",
    "SEED      = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    \"\"\"Carrega modelo e tokenizer do Hugging Face, com cache local.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()  # Desativa dropout e outras varia√ß√µes estoc√°sticas\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def make_sentences(n: int) -> List[str]:\n",
    "    \"\"\"Gera *n* frases distintas simulando avalia√ß√µes de evento.\"\"\"\n",
    "    templates = [\n",
    "        \"Estou muito animado para participar da XXV ERAD/RS 2025 em Foz do Igua√ßu! - Exemplo {}\",\n",
    "        \"Infelizmente perdi o prazo de submiss√£o de trabalhos para a ERAD, fiquei desapontado. - Exemplo {}\",\n",
    "        \"O cronograma de minicursos sobre computa√ß√£o exaescala parece excelente. - Exemplo {}\",\n",
    "        \"A prorroga√ß√£o das inscri√ß√µes de novo foi frustrante para alguns participantes. - Exemplo {}\",\n",
    "        \"O F√≥rum de Inicia√ß√£o Cient√≠fica da ERAD/RS costuma ser inspirador e motivador. - Exemplo {}\"\n",
    "    ]\n",
    "    return [random.choice(templates).format(i) for i in range(n)]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(sentences: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Executa a infer√™ncia uma por uma na CPU.\n",
    "    N√£o h√° paralelismo entre exemplos.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for s in sentences:\n",
    "        inputs = tokenizer(s, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        _ = model(**inputs)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(sentences: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Executa infer√™ncia em lote na GPU.\n",
    "    Aqui ocorre paralelismo verdadeiro: o modelo processa todas as entradas simultaneamente\n",
    "    usando os n√∫cleos CUDA da GPU, com alto aproveitamento computacional.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        # Tokeniza√ß√£o em lote com padding para alinhamento\n",
    "        inputs = tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # Garante que nada esteja pendente\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        # Aqui o paralelismo entra em a√ß√£o: todos os inputs s√£o processados juntos\n",
    "        _ = model(**inputs)\n",
    "\n",
    "        torch.cuda.synchronize()  # Espera o fim da execu√ß√£o dos kernels\n",
    "        return time.perf_counter() - t0\n",
    "    finally:\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Loop principal do benchmark.\n",
    "    Para cada tamanho de batch definido em SIZES:\n",
    "    - Gera frases de entrada\n",
    "    - Executa em CPU e GPU\n",
    "    - Registra e imprime tempo + acelera√ß√£o relativa\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        samples = make_sentences(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(samples)\n",
    "        t_gpu = run_gpu_batch(samples)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02598a2e-599c-4b97-94b9-4be99f227076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#3 - Sumariza√ß√£o de Texto com Modelo PT-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f375e2-16c2-44e4-9b76-f02fd037b5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio demonstra como utilizar modelos de sequ√™ncia para sequ√™ncia (seq2seq) para gerar resumos autom√°ticos de textos longos em portugu√™s, com foco em efici√™ncia. S√£o aplicadas t√©cnicas de infer√™ncia em lote (batch inference) usando GPU, aproveitando o paralelismo de hardware para acelerar a tarefa.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "A sumariza√ß√£o autom√°tica tem ampla aplica√ß√£o em:\n",
    "- Gera√ß√£o de resumos de not√≠cias, artigos acad√™micos ou documentos corporativos.\n",
    "- Assistentes virtuais que oferecem resumos r√°pidos de conte√∫dos longos.\n",
    "- Ferramentas de produtividade com foco em leitura r√°pida ou extra√ß√£o de informa√ß√µes.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "- Input: Lista de textos longos (strings em portugu√™s) ‚Äî cada um com cerca de 3 a 6 frases.\n",
    "- Output: Um resumo autom√°tico para cada entrada, com no m√°ximo 64 tokens gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7518b423-32ae-499e-accc-6566a6d0e779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7412f26ed67f4359930ef8cd1eebd7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e30dc108784c0e849a50a03418752f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38588dc6bd7e4973aa58eaae942e8241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/756k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392e1b59fc694a5a8d21210153674586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nTensor Parallel requires torch.distributed to be initialized first.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95291ff25224e438cd9bc8e2a43092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== PT-T5 ‚Ä¢ sumariza√ß√£o batelada (GPU) ‚Äî 0.60 s ===\n\n‚îÄ‚îÄ Texto 1 (origem ERAD/RS) ‚îÄ‚îÄ\nA XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) ocorrer√° entre 23 e 25 de abril em Foz do Igua√ßu, Paran√°. O evento comemora 25 anos promovendo cursos, palestras e desafios pr√°ticos sobre computa√ß√£o de alto desempenho, intelig√™ncia artificial e sistemas paralelos. Nesta edi√ß√£o, a programa√ß√£o inclui visita t√©cnica √† Itaipu Parquetec e pain√©is dedicados a exaescala e sustentabilidade energ√©tica.\n\n‚ñ∏ Resumo:\nA XXV Escola Regional de Alto Desempenho da Regi√£o Sul ocorrer√° entre 23 e 25 de abril em Foz do Igua√ßu.\n\n‚îÄ‚îÄ Texto 2 (origem ERAD/RS) ‚îÄ‚îÄ\nO Comit√™ de Programa da ERAD/RS convida estudantes e pesquisadores a submeterem trabalhos para o F√≥rum de Inicia√ß√£o Cient√≠fica. Ser√£o aceitos artigos de at√© oito p√°ginas descrevendo resultados parciais ou finais em processamento de alto desempenho, arquitetura de GPUs, compiladores e aplica√ß√µes cient√≠ficas. Os autores dos melhores artigos ganhar√£o inscri√ß√£o gratuita e premia√ß√£o em dinheiro.\n\n‚ñ∏ Resumo:\nCandidate estudantes e pesquisadores a submeterem trabalhos para o F√≥rum de Inicia√ß√£o Cient√≠fica.\n\n‚îÄ‚îÄ Texto 3 (origem ERAD/RS) ‚îÄ‚îÄ\nAl√©m dos minicursos tradicionais, a organiza√ß√£o oferecer√° um hackathon sobre otimiza√ß√£o de kernels CUDA, com premia√ß√£o para a equipe que alcan√ßar maior acelera√ß√£o no benchmark de simula√ß√£o clim√°tica. Os participantes ter√£o acesso a clusters com placas NVIDIA A100 e suporte de mentores da ind√∫stria.\n\n‚ñ∏ Resumo:\nAl√©m dos minicursos tradicionais, a organiza√ß√£o oferecer√° um hackathon sobre otimiza√ß√£o de kernels CUDA.\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ÉO GERAL\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID  = \"recogna-nlp/ptt5-base-summ-cstnews\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW   = 64  # M√°ximo de tokens gerados por resumo\n",
    "\n",
    "# Exemplos reais adaptados do site da ERAD/RS 2025\n",
    "TEXTOS_LONGOS = [\n",
    "    (\n",
    "        \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) \"\n",
    "        \"ocorrer√° entre 23 e 25 de abril em Foz do Igua√ßu, Paran√°. O evento \"\n",
    "        \"comemora 25 anos promovendo cursos, palestras e desafios pr√°ticos sobre \"\n",
    "        \"computa√ß√£o de alto desempenho, intelig√™ncia artificial e sistemas \"\n",
    "        \"paralelos. Nesta edi√ß√£o, a programa√ß√£o inclui visita t√©cnica √† Itaipu \"\n",
    "        \"Parquetec e pain√©is dedicados a exaescala e sustentabilidade energ√©tica.\"\n",
    "    ),\n",
    "    (\n",
    "        \"O Comit√™ de Programa da ERAD/RS convida estudantes e pesquisadores a \"\n",
    "        \"submeterem trabalhos para o F√≥rum de Inicia√ß√£o Cient√≠fica. Ser√£o aceitos \"\n",
    "        \"artigos de at√© oito p√°ginas descrevendo resultados parciais ou finais \"\n",
    "        \"em processamento de alto desempenho, arquitetura de GPUs, compiladores \"\n",
    "        \"e aplica√ß√µes cient√≠ficas. Os autores dos melhores artigos ganhar√£o \"\n",
    "        \"inscri√ß√£o gratuita e premia√ß√£o em dinheiro.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Al√©m dos minicursos tradicionais, a organiza√ß√£o oferecer√° um hackathon \"\n",
    "        \"sobre otimiza√ß√£o de kernels CUDA, com premia√ß√£o para a equipe que \"\n",
    "        \"alcan√ßar maior acelera√ß√£o no benchmark de simula√ß√£o clim√°tica. Os \"\n",
    "        \"participantes ter√£o acesso a clusters com placas NVIDIA A100 e suporte \"\n",
    "        \"de mentores da ind√∫stria.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO DE CARREGAMENTO DO MODELO\n",
    "# ------------------------------------------------------------------------------\n",
    "def carregar_modelo(model_id: str):\n",
    "    \"\"\"\n",
    "    Carrega tokenizer e modelo para sumariza√ß√£o.\n",
    "    O modelo √© movido automaticamente para a GPU (device_map=\"auto\")\n",
    "    e usa precis√£o reduzida (float16) para desempenho otimizado.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ CUDA n√£o encontrada ‚Äî a vers√£o GPU exige placa NVIDIA.\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"  # Distribui o modelo automaticamente na GPU dispon√≠vel\n",
    "    )\n",
    "    return tok, model\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO PRINCIPAL DE INFER√äNCIA EM LOTE\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def resumir_batch(textos: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Realiza sumariza√ß√£o de todos os textos de uma vez,\n",
    "    aproveitando o paralelismo da GPU via infer√™ncia batelada (batch).\n",
    "    \"\"\"\n",
    "    tok, model = carregar_modelo(MODEL_ID)\n",
    "\n",
    "    # Tokeniza√ß√£o com padding e truncamento para que todas as entradas\n",
    "    # fiquem no mesmo tamanho e possam ser processadas em lote (paralelo)\n",
    "    inputs = tok(textos, return_tensors=\"pt\", padding=True,\n",
    "                 truncation=True, max_length=512).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "    torch.cuda.synchronize()  # Garante que GPU esteja ociosa antes da medi√ß√£o\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # A gera√ß√£o ocorre com num_beams > 1 (beam search), ativando heur√≠stica de qualidade\n",
    "    # O paralelismo ocorre aqui: todos os textos s√£o resumidos em uma √∫nica chamada de GPU\n",
    "    outs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    print(f\"\\n=== PT-T5 ‚Ä¢ sumariza√ß√£o batelada (GPU) ‚Äî {elapsed:.2f} s ===\\n\")\n",
    "\n",
    "    # Impress√£o dos resultados\n",
    "    for idx, (orig, out_ids) in enumerate(zip(textos, outs), 1):\n",
    "        resumo = tok.decode(out_ids, skip_special_tokens=True)\n",
    "        print(f\"‚îÄ‚îÄ Texto {idx} (origem ERAD/RS) ‚îÄ‚îÄ\\n{orig}\\n\")\n",
    "        print(f\"‚ñ∏ Resumo:\\n{resumo}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# PONTO DE ENTRADA\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    resumir_batch(TEXTOS_LONGOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadc4647-07aa-4716-a39c-2ee586845d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bbddc5604949908d3bb5b483e4f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eee838194694db8b98f87a051115180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e1f81d566a43ba978525683a22e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/756k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0dc89543e4426b85838fd891141069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abc2ef0a7fa4bfcbe89dad1ea6233f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    N | CPU-SEQ (s) |  GPU (s) | Speed-up\n-------------------------------------------\n    1 |        0.86 |     0.55 |   1.57√ó\n    2 |        2.23 |     0.86 |   2.58√ó\n    4 |        4.60 |     0.97 |   4.72√ó\n    8 |        7.88 |     1.08 |   7.27√ó\n   16 |       14.15 |     1.39 |  10.17√ó\n   32 |       27.34 |     2.29 |  11.91√ó\n   64 |       52.99 |     4.29 |  12.36√ó\n  128 |      106.61 |     8.43 |  12.65√ó\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:737)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:81)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:81)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:715)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Dict\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ÉO GLOBAL DO EXPERIMENTO\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID   = \"recogna-nlp/ptt5-base-summ-cstnews\"\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW    = 64                          # Limite de tokens a serem gerados\n",
    "SIZES      = [2**i for i in range(8)]\n",
    "SEED       = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    \"\"\"Carrega o modelo e tokenizer do Hugging Face com cache local.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def make_articles(n: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gera *n* artigos longos para sumariza√ß√£o.\n",
    "    A entrada usa o prefixo 'summarize:' pois T5 espera instru√ß√µes expl√≠citas.\n",
    "    \"\"\"\n",
    "    base = (\n",
    "        \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) \"\n",
    "        \"ocorrer√° entre 23 e 25 de abril em Foz do Igua√ßu, Paran√°. O evento \"\n",
    "        \"comemora 25 anos promovendo cursos, palestras e desafios pr√°ticos sobre \"\n",
    "        \"computa√ß√£o de alto desempenho, intelig√™ncia artificial e sistemas \"\n",
    "        \"paralelos. Nesta edi√ß√£o, a programa√ß√£o inclui visita t√©cnica √† Itaipu \"\n",
    "        \"Parquetec e pain√©is dedicados a exaescala e sustentabilidade energ√©tica.\"\n",
    "    )\n",
    "    return [f\"summarize: {base} Sample number {i}.\" for i in range(n)]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(inputs: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Sumariza os textos um a um usando CPU.\n",
    "    N√£o h√° paralelismo entre inputs, resultando em maior tempo total.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for text in inputs:\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(\"cpu\")\n",
    "        _ = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            do_sample=False,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(inputs: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Sumariza todo o lote de textos simultaneamente usando GPU.\n",
    "    Aqui √© ativado o paralelismo total ‚Äî m√∫ltiplas sequ√™ncias s√£o processadas\n",
    "    de forma vetorizada usando os n√∫cleos CUDA da GPU.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "        tokens = tokenizer(inputs, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # garante que a GPU est√° ociosa antes de medir\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        _ = model.generate(\n",
    "            **tokens,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            do_sample=False,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()  # aguarda todos os kernels terminarem\n",
    "        return time.perf_counter() - t0\n",
    "\n",
    "    finally:\n",
    "        del tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Loop principal de benchmark:\n",
    "    - Executa sumariza√ß√£o para diferentes tamanhos de lote\n",
    "    - Compara execu√ß√£o sequencial na CPU vs. paralela na GPU\n",
    "    - Exibe uma tabela com tempos absolutos e speed-up relativo\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        prompts = make_articles(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(prompts)\n",
    "        t_gpu = run_gpu_batch(prompts)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7cd4653-1224-4ac8-aa17-1c3a35618476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#4 - Tradu√ß√£o com mT5: Ingl√™s para Portugu√™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8529b179-c602-409c-89ea-10987ea9654c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio realiza a tradu√ß√£o autom√°tica de frases em ingl√™s para o portugu√™s usando um modelo mT5-base especializado no par lingu√≠stico en‚Üípt. A execu√ß√£o em lote na GPU demonstra como tarefas de NLP com modelos grandes (3B par√¢metros) podem ser aceleradas com paralelismo de hardware.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Aplica√ß√µes pr√°ticas incluem:\n",
    "- Tradu√ß√£o autom√°tica em sistemas de atendimento multil√≠ngue.\n",
    "- Ferramentas de localiza√ß√£o e globaliza√ß√£o de conte√∫do.\n",
    "- Sistemas de recomenda√ß√£o e resumo de conte√∫do internacional.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "- Input: Lista de frases em ingl√™s (strings).\n",
    "- Output: Lista de tradu√ß√µes para o portugu√™s (strings geradas pelo modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584c6bf8-6571-4ffb-8492-05bbe8fff390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== mT5-base EN‚ÜíPT ‚Ä¢ tradu√ß√£o batelada (GPU) ‚Äî 0.24 s ===\n\nEN: The XXV ERAD/RS 2025 will be held in Foz do Iguacu between April 23 and 25.\nPT: true\n\nEN: Participants can visit the Itaipu Parquetec facilities during the event.\nPT: true\n\nEN: The program features advanced courses on GPU programming and exascale computing.\nPT: true\n\nEN: The deadline for paper submissions to the Scientific Initiation Forum was extended again.\nPT: false\n\nEN: Winners of the CUDA kernel hackathon will receive cash prizes and free registration.\nPT: true\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5741184051847171>, line 90\u001B[0m\n",
       "\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# PONTO DE ENTRADA\u001B[39;00m\n",
       "\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m---> 90\u001B[0m     translate_batch(EN_SENTENCES)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n",
       "\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m<command-5741184051847171>, line 51\u001B[0m, in \u001B[0;36mtranslate_batch\u001B[0;34m(sentences)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslate_batch\u001B[39m(sentences: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m    Executa tradu√ß√£o de todas as frases em lote.\u001B[39;00m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Isso ativa o paralelismo da GPU ‚Äî todas as entradas s√£o processadas simultaneamente.\u001B[39;00m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 51\u001B[0m     tok, model \u001B[38;5;241m=\u001B[39m load_pipeline(MODEL_ID)\n",
       "\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# Tokeniza√ß√£o vetorizada com padding e truncamento para uniformizar o batch\u001B[39;00m\n",
       "\u001B[1;32m     54\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tok(\n",
       "\u001B[1;32m     55\u001B[0m         sentences,\n",
       "\u001B[1;32m     56\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     59\u001B[0m         max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n",
       "\u001B[1;32m     60\u001B[0m     )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m<command-5741184051847171>, line 38\u001B[0m, in \u001B[0;36mload_pipeline\u001B[0;34m(model_id)\u001B[0m\n",
       "\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müö´ Esta vers√£o exige GPU CUDA ‚Äî nenhuma placa detectada.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     32\u001B[0m tok \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n",
       "\u001B[1;32m     33\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSeq2SeqLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n",
       "\u001B[1;32m     34\u001B[0m     model_id,\n",
       "\u001B[1;32m     35\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR,\n",
       "\u001B[1;32m     36\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,  \u001B[38;5;66;03m# Ativa FP16 (meia precis√£o)\u001B[39;00m\n",
       "\u001B[1;32m     37\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m           \u001B[38;5;66;03m# Coloca o modelo inteiro na GPU dispon√≠vel\u001B[39;00m\n",
       "\u001B[0;32m---> 38\u001B[0m )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tok, model\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/modeling_utils.py:3712\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n",
       "\u001B[1;32m   3708\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   3709\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   3710\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   3711\u001B[0m         )\n",
       "\u001B[0;32m-> 3712\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1340\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1341\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\u001B[0;32m-> 1343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n",
       "\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n",
       "\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n",
       "\u001B[0;32m--> 903\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n",
       "\u001B[1;32m    905\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n",
       "\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
       "\u001B[1;32m    907\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n",
       "\u001B[1;32m    908\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    913\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n",
       "\u001B[1;32m    914\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n",
       "\u001B[1;32m    926\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n",
       "\u001B[1;32m    927\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n",
       "\u001B[1;32m    928\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n",
       "\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
       "\u001B[0;32m--> 930\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n",
       "\u001B[1;32m    931\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n",
       "\u001B[1;32m    933\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n",
       "\u001B[1;32m   1322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n",
       "\u001B[1;32m   1323\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n",
       "\u001B[1;32m   1324\u001B[0m             device,\n",
       "\u001B[1;32m   1325\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1326\u001B[0m             non_blocking,\n",
       "\u001B[1;32m   1327\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n",
       "\u001B[1;32m   1328\u001B[0m         )\n",
       "\u001B[0;32m-> 1329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n",
       "\u001B[1;32m   1330\u001B[0m         device,\n",
       "\u001B[1;32m   1331\u001B[0m         dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1332\u001B[0m         non_blocking,\n",
       "\u001B[1;32m   1333\u001B[0m     )\n",
       "\u001B[1;32m   1334\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 7.56 MiB is free. Process 13365 has 14.57 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 75.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "OutOfMemoryError",
        "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 7.56 MiB is free. Process 13365 has 14.57 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 75.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-5741184051847171>, line 90\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# PONTO DE ENTRADA\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 90\u001B[0m     translate_batch(EN_SENTENCES)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m<command-5741184051847171>, line 51\u001B[0m, in \u001B[0;36mtranslate_batch\u001B[0;34m(sentences)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslate_batch\u001B[39m(sentences: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124;03m    Executa tradu√ß√£o de todas as frases em lote.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Isso ativa o paralelismo da GPU ‚Äî todas as entradas s√£o processadas simultaneamente.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     tok, model \u001B[38;5;241m=\u001B[39m load_pipeline(MODEL_ID)\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# Tokeniza√ß√£o vetorizada com padding e truncamento para uniformizar o batch\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tok(\n\u001B[1;32m     55\u001B[0m         sentences,\n\u001B[1;32m     56\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     59\u001B[0m         max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m     60\u001B[0m     )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m<command-5741184051847171>, line 38\u001B[0m, in \u001B[0;36mload_pipeline\u001B[0;34m(model_id)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124müö´ Esta vers√£o exige GPU CUDA ‚Äî nenhuma placa detectada.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     32\u001B[0m tok \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id, cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR)\n\u001B[1;32m     33\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSeq2SeqLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     34\u001B[0m     model_id,\n\u001B[1;32m     35\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mCACHE_DIR,\n\u001B[1;32m     36\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,  \u001B[38;5;66;03m# Ativa FP16 (meia precis√£o)\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m           \u001B[38;5;66;03m# Coloca o modelo inteiro na GPU dispon√≠vel\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tok, model\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/modeling_utils.py:3712\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[1;32m   3708\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3709\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3710\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3711\u001B[0m         )\n\u001B[0;32m-> 3712\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1340\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1341\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 903\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[1;32m    905\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    907\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    908\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    913\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    914\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 930\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n\u001B[1;32m    931\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    933\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1323\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1324\u001B[0m             device,\n\u001B[1;32m   1325\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1326\u001B[0m             non_blocking,\n\u001B[1;32m   1327\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1328\u001B[0m         )\n\u001B[0;32m-> 1329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1330\u001B[0m         device,\n\u001B[1;32m   1331\u001B[0m         dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1332\u001B[0m         non_blocking,\n\u001B[1;32m   1333\u001B[0m     )\n\u001B[1;32m   1334\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
        "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 7.56 MiB is free. Process 13365 has 14.57 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 75.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ÉO GLOBAL\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID  = \"unicamp-dl/mt5-3B-mmarco-en-pt\"  # Modelo mT5 treinado para EN‚ÜíPT\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW   = 96  # Tokens m√°ximos a serem gerados por tradu√ß√£o\n",
    "\n",
    "# Frases de entrada (retiradas ou inspiradas do site da ERAD/RS 2025)\n",
    "EN_SENTENCES = [\n",
    "    \"The XXV ERAD/RS 2025 will be held in Foz do Iguacu between April 23 and 25.\",\n",
    "    \"Participants can visit the Itaipu Parquetec facilities during the event.\",\n",
    "    \"The program features advanced courses on GPU programming and exascale computing.\",\n",
    "    \"The deadline for paper submissions to the Scientific Initiation Forum was extended again.\",\n",
    "    \"Winners of the CUDA kernel hackathon will receive cash prizes and free registration.\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO DE CARREGAMENTO DO MODELO\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_pipeline(model_id: str):\n",
    "    \"\"\"\n",
    "    Carrega modelo e tokenizer diretamente na GPU, com uso de float16 (meia precis√£o).\n",
    "    Isso reduz uso de VRAM e acelera as opera√ß√µes em hardwares modernos como T4, A10, A100.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ Esta vers√£o exige GPU CUDA ‚Äî nenhuma placa detectada.\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        torch_dtype=torch.float16,  # Ativa FP16 (meia precis√£o)\n",
    "        device_map=\"auto\"           # Coloca o modelo inteiro na GPU dispon√≠vel\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return tok, model\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO DE INFER√äNCIA EM LOTE\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def translate_batch(sentences: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Executa tradu√ß√£o de todas as frases em lote.\n",
    "    Isso ativa o paralelismo da GPU ‚Äî todas as entradas s√£o processadas simultaneamente.\n",
    "    \"\"\"\n",
    "    tok, model = load_pipeline(MODEL_ID)\n",
    "\n",
    "    # Tokeniza√ß√£o vetorizada com padding e truncamento para uniformizar o batch\n",
    "    inputs = tok(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "    # Mede tempo de infer√™ncia com sincroniza√ß√£o antes e depois\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Gera√ß√£o paralela das tradu√ß√µes via beam search (melhor qualidade)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    # Decodifica as sa√≠das para texto em portugu√™s\n",
    "    pt_texts = tok.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Impress√£o dos resultados\n",
    "    print(f\"\\n=== mT5-base EN‚ÜíPT ‚Ä¢ tradu√ß√£o batelada (GPU) ‚Äî {elapsed:.2f} s ===\\n\")\n",
    "    for en, pt in zip(sentences, pt_texts):\n",
    "        print(f\"EN: {en}\\nPT: {pt}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# PONTO DE ENTRADA\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    translate_batch(EN_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16f28ba-7cc9-4c8b-88e6-c0d205429860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 11:25:34.449262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-25 11:25:34.465378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-25 11:25:34.485401: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-25 11:25:34.491454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-25 11:25:34.506083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-25 11:25:35.571919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ace26a9d204067b6928c2088e035ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba00733a13c4dbb9d26cc3e5987a380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff568831533e4ac19cb4a4b5c8e67039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3e190abf024041bfa263bb9387038d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50112308abc24446ba145b9489e63926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5222b4c5a734eab8ac0d962f55a3b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    N | CPU-SEQ (s) |  GPU (s) | Speed-up\n-------------------------------------------\n    1 |        0.10 |     0.85 |   0.12√ó\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 |        0.18 |     0.06 |   3.13√ó\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4 |        0.33 |     0.06 |   5.23√ó\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 |        0.55 |     0.06 |   8.98√ó\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=64) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 |        1.13 |     0.06 |  17.68√ó\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5741184051847141>, line 126\u001B[0m\n",
       "\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n",
       "\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 126\u001B[0m     benchmark()\n",
       "\n",
       "File \u001B[0;32m<command-5741184051847141>, line 113\u001B[0m, in \u001B[0;36mbenchmark\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    110\u001B[0m prompts \u001B[38;5;241m=\u001B[39m make_sentences(n)\n",
       "\u001B[1;32m    112\u001B[0m t_cpu \u001B[38;5;241m=\u001B[39m run_cpu_seq(prompts)\n",
       "\u001B[0;32m--> 113\u001B[0m t_gpu \u001B[38;5;241m=\u001B[39m run_gpu_batch(prompts)\n",
       "\u001B[1;32m    115\u001B[0m speed \u001B[38;5;241m=\u001B[39m (t_cpu \u001B[38;5;241m/\u001B[39m t_gpu) \u001B[38;5;28;01mif\u001B[39;00m (t_gpu \u001B[38;5;129;01mand\u001B[39;00m t_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    116\u001B[0m results[n] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_cpu, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_gpu, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspeedup\u001B[39m\u001B[38;5;124m\"\u001B[39m: speed}\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n",
       "\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m<command-5741184051847141>, line 78\u001B[0m, in \u001B[0;36mrun_gpu_batch\u001B[0;34m(sentences)\u001B[0m\n",
       "\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 78\u001B[0m     model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     79\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tokenizer(sentences, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     81\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()  \u001B[38;5;66;03m# sincronia antes da medi√ß√£o\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/modeling_utils.py:3712\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n",
       "\u001B[1;32m   3708\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   3709\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   3710\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   3711\u001B[0m         )\n",
       "\u001B[0;32m-> 3712\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1340\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1341\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\u001B[0;32m-> 1343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n",
       "\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n",
       "\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n",
       "\u001B[0;32m--> 903\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n",
       "\u001B[1;32m    905\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n",
       "\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
       "\u001B[1;32m    907\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n",
       "\u001B[1;32m    908\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    913\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n",
       "\u001B[1;32m    914\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n",
       "\u001B[1;32m    926\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n",
       "\u001B[1;32m    927\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n",
       "\u001B[1;32m    928\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n",
       "\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
       "\u001B[0;32m--> 930\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n",
       "\u001B[1;32m    931\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n",
       "\u001B[1;32m    933\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n",
       "\u001B[1;32m   1322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n",
       "\u001B[1;32m   1323\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n",
       "\u001B[1;32m   1324\u001B[0m             device,\n",
       "\u001B[1;32m   1325\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1326\u001B[0m             non_blocking,\n",
       "\u001B[1;32m   1327\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n",
       "\u001B[1;32m   1328\u001B[0m         )\n",
       "\u001B[0;32m-> 1329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n",
       "\u001B[1;32m   1330\u001B[0m         device,\n",
       "\u001B[1;32m   1331\u001B[0m         dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1332\u001B[0m         non_blocking,\n",
       "\u001B[1;32m   1333\u001B[0m     )\n",
       "\u001B[1;32m   1334\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\n",
       "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 15.56 MiB is free. Process 26596 has 14.56 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 73.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "OutOfMemoryError",
        "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 15.56 MiB is free. Process 26596 has 14.56 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 73.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-5741184051847141>, line 126\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# ------------------------------------------------------------------------------\u001B[39;00m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 126\u001B[0m     benchmark()\n",
        "File \u001B[0;32m<command-5741184051847141>, line 113\u001B[0m, in \u001B[0;36mbenchmark\u001B[0;34m()\u001B[0m\n\u001B[1;32m    110\u001B[0m prompts \u001B[38;5;241m=\u001B[39m make_sentences(n)\n\u001B[1;32m    112\u001B[0m t_cpu \u001B[38;5;241m=\u001B[39m run_cpu_seq(prompts)\n\u001B[0;32m--> 113\u001B[0m t_gpu \u001B[38;5;241m=\u001B[39m run_gpu_batch(prompts)\n\u001B[1;32m    115\u001B[0m speed \u001B[38;5;241m=\u001B[39m (t_cpu \u001B[38;5;241m/\u001B[39m t_gpu) \u001B[38;5;28;01mif\u001B[39;00m (t_gpu \u001B[38;5;129;01mand\u001B[39;00m t_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m results[n] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_cpu, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m: t_gpu, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspeedup\u001B[39m\u001B[38;5;124m\"\u001B[39m: speed}\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m<command-5741184051847141>, line 78\u001B[0m, in \u001B[0;36mrun_gpu_batch\u001B[0;34m(sentences)\u001B[0m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 78\u001B[0m     model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     79\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tokenizer(sentences, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     81\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()  \u001B[38;5;66;03m# sincronia antes da medi√ß√£o\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/transformers/modeling_utils.py:3712\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[1;32m   3708\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3709\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3710\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3711\u001B[0m         )\n\u001B[0;32m-> 3712\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1340\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1341\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 903\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[1;32m    905\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    906\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    907\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    908\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    913\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    914\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    929\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 930\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n\u001B[1;32m    931\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    933\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1323\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1324\u001B[0m             device,\n\u001B[1;32m   1325\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1326\u001B[0m             non_blocking,\n\u001B[1;32m   1327\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1328\u001B[0m         )\n\u001B[0;32m-> 1329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1330\u001B[0m         device,\n\u001B[1;32m   1331\u001B[0m         dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1332\u001B[0m         non_blocking,\n\u001B[1;32m   1333\u001B[0m     )\n\u001B[1;32m   1334\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
        "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 15.56 MiB is free. Process 26596 has 14.56 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 73.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Dict\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ïES DO EXPERIMENTO\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID   = \"unicamp-dl/ptt5-base-pt-msmarco-10k-v2\"\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MAX_NEW    = 64                     # Tokens m√°ximos por tradu√ß√£o\n",
    "SIZES      = [2**i for i in range(5)]  # N de senten√ßas: 1 a 32\n",
    "SEED       = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    \"\"\"\n",
    "    Baixa e carrega o modelo + tokenizer com cache local.\n",
    "    N√£o faz distin√ß√£o de CPU ou GPU aqui ‚Äî isso √© feito durante a execu√ß√£o.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def make_sentences(n: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gera N senten√ßas √∫nicas em ingl√™s para simular um cen√°rio de tradu√ß√£o real.\n",
    "    Isso evita reuso de tokens e for√ßa o modelo a processar cada entrada.\n",
    "    \"\"\"\n",
    "    templates = [\n",
    "        \"Machine translation is a fundamental task in NLP, sample {}.\",\n",
    "        \"This paper explores the role of deep learning in language processing, example {}.\",\n",
    "        \"Translation models must handle linguistic variation, instance {}.\",\n",
    "        \"Modern neural models outperform traditional methods, item {}.\",\n",
    "        \"Language pairs with less data pose unique challenges, case {}.\"\n",
    "    ]\n",
    "    return [random.choice(templates).format(i) for i in range(n)]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(sentences: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Traduz sequencialmente cada senten√ßa na CPU.\n",
    "    Este m√©todo n√£o se beneficia de paralelismo, sendo √∫til como baseline.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(\"cpu\")\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=64\n",
    "        )\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(sentences: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Traduz todas as senten√ßas de uma vez na GPU.\n",
    "    Esta abordagem explora o paralelismo natural das GPUs via execu√ß√£o batelada.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # sincronia antes da medi√ß√£o\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()  # sincronia ap√≥s gera√ß√£o\n",
    "        return time.perf_counter() - t0\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Executa o benchmark para diferentes tamanhos de batch.\n",
    "    Para cada N, mede tempo total na CPU (sequencial) e GPU (paralelo),\n",
    "    calculando o speed-up obtido com computa√ß√£o de alto desempenho.\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        prompts = make_sentences(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(prompts)\n",
    "        t_gpu = run_gpu_batch(prompts)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a34a2578-94e7-41cf-98cf-4b41e9ea97fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#5 - Perguntas e Respostas em Portugu√™s com BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff79caf-5d9d-41d0-8b63-481e1291d08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio realiza infer√™ncia batelada (em lote) de perguntas-e-respostas (QA) com um modelo BERT treinado em portugu√™s. Os pares (contexto, pergunta) s√£o processados simultaneamente na GPU, e o modelo retorna o trecho exato que responde √† pergunta, com base em logits de in√≠cio e fim dos tokens.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Aplica√ß√µes pr√°ticas incluem:\n",
    "- Sistemas de FAQ autom√°ticos em portugu√™s\n",
    "- Chatbots com compreens√£o de contexto\n",
    "- Sistemas de busca por resposta exata (ex: jur√≠dico, educacional)\n",
    "- Acelera√ß√£o de pipelines de QA em ambientes de produ√ß√£o\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "- Input: Lista de pares (contexto, pergunta) em portugu√™s.\n",
    "- Output: Lista de respostas extra√≠das diretamente dos contextos ‚Äî respostas curtas, objetivas, com base textual (span-based QA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5ebfd0-9e9f-4084-8e3e-6f8dab506aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ÉO DO MODELO E CACHE\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID  = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DADOS DE ENTRADA (Contextos reais da ERAD/RS + perguntas correspondentes)\n",
    "# ------------------------------------------------------------------------------\n",
    "QA_PAIRS = [\n",
    "    (\n",
    "        \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) \"\n",
    "        \"acontecer√° de 23 a 25 de abril em Foz do Igua√ßu, no Paran√°. O evento \"\n",
    "        \"re√∫ne minicursos, palestras e uma visita t√©cnica ao Itaipu Parquetec.\",\n",
    "        \"Em que cidade ser√° realizada a ERAD/RS 2025?\"\n",
    "    ),\n",
    "    (\n",
    "        \"O Comit√™ de Programa da ERAD/RS prorrogou o prazo de submiss√£o de \"\n",
    "        \"trabalhos para o F√≥rum de Inicia√ß√£o Cient√≠fica. Os artigos podem ter \"\n",
    "        \"at√© oito p√°ginas e devem abordar temas de computa√ß√£o de alto desempenho.\",\n",
    "        \"Qual foi o prazo prorrogado pela organiza√ß√£o?\"\n",
    "    ),\n",
    "    (\n",
    "        \"Durante a ERAD/RS 2025 haver√° um hackathon focado em otimiza√ß√£o de \"\n",
    "        \"kernels CUDA, utilizando clusters com GPUs NVIDIA A100. As melhores \"\n",
    "        \"equipes receber√£o pr√™mios em dinheiro.\",\n",
    "        \"Qual tecnologia de GPU ser√° usada no hackathon?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# FUN√á√ÉO DE CARREGAMENTO DO MODELO\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Carrega tokenizer e modelo diretamente para GPU.\n",
    "    Utiliza FP16 para otimizar mem√≥ria e acelerar a infer√™ncia.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ Esta vers√£o exige GPU CUDA ‚Äî nenhuma placa detectada.\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        torch_dtype=torch.float16  # usa meia precis√£o para maior efici√™ncia\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return tok, model\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# INFER√äNCIA EM LOTE NA GPU\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def answer_batch(pairs):\n",
    "    \"\"\"\n",
    "    Executa QA em lote, aproveitando o paralelismo da GPU.\n",
    "    Cada par (contexto, pergunta) √© processado simultaneamente.\n",
    "    \"\"\"\n",
    "    tok, model = load_model(MODEL_ID)\n",
    "\n",
    "    # Separa√ß√£o dos pares em listas de perguntas e contextos\n",
    "    questions, contexts = zip(*pairs)\n",
    "\n",
    "    # Tokeniza√ß√£o paralela e envio para a GPU\n",
    "    inputs = tok(\n",
    "        list(questions),\n",
    "        list(contexts),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Medi√ß√£o de tempo com sincroniza√ß√£o expl√≠cita\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    outputs = model(**inputs)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    # Inicializa vetor de respostas\n",
    "    answers = [\"\"] * len(pairs)\n",
    "\n",
    "    # Extra√ß√£o do trecho com maior pontua√ß√£o entre os logits de in√≠cio e fim\n",
    "    for i in range(len(pairs)):\n",
    "        start_idx = int(torch.argmax(outputs.start_logits[i]))\n",
    "        end_idx   = int(torch.argmax(outputs.end_logits[i])) + 1\n",
    "        tokens    = inputs[\"input_ids\"][i][start_idx:end_idx]\n",
    "        answers[i] = tok.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Impress√£o dos resultados\n",
    "    print(f\"\\n=== BERT-SQuAD-PT ‚Ä¢ QA batelada (GPU) ‚Äî {elapsed:.2f} s ===\\n\")\n",
    "    for (ctx, q), ans in zip(pairs, answers):\n",
    "        print(f\"Pergunta : {q}\\nResposta  : {ans}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# PONTO DE ENTRADA\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    answer_batch(QA_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4fbe6a9-93d5-4cf6-90a3-3e048b16ae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURA√á√ïES DO EXPERIMENTO\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_ID   = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "SIZES      = [2**i for i in range(9)]  # N de pares (1 at√© 256)\n",
    "SEED       = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    \"\"\"\n",
    "    Carrega tokenizer e modelo da Hugging Face com cache local.\n",
    "    A escolha do dispositivo (CPU/GPU) √© feita posteriormente na execu√ß√£o.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def make_qa_pairs(n: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Gera N pares de (contexto, pergunta) com varia√ß√µes na pergunta.\n",
    "    O contexto √© sempre o mesmo para permitir compara√ß√£o justa de performance.\n",
    "    \"\"\"\n",
    "    base_context = (\n",
    "        \"A XXV Escola Regional de Alto Desempenho da Regi√£o Sul (ERAD/RS 2025) acontecer√° de 23 a 25 de abril em Foz do Igua√ßu, no Paran√°. O evento re√∫ne minicursos, palestras e uma visita t√©cnica ao Itaipu Parquetec.\",\n",
    "        \"O Comit√™ de Programa da ERAD/RS prorrogou o prazo de submiss√£o de trabalhos para o F√≥rum de Inicia√ß√£o Cient√≠fica. Os artigos podem ter at√© oito p√°ginas e devem abordar temas de computa√ß√£o de alto desempenho.\",\n",
    "        \"Durante a ERAD/RS 2025 haver√° um hackathon focado em otimiza√ß√£o de kernels CUDA, utilizando clusters com GPUs NVIDIA A100. As melhores equipes receber√£o pr√™mios em dinheiro.\",\n",
    "    )\n",
    "    questions = [\n",
    "        \"Qual tecnologia de GPU ser√° usada no hackathon?\",\n",
    "        \"Qual foi o prazo prorrogado pela organiza√ß√£o?\",\n",
    "        \"Em que cidade ser√° realizada a ERAD/RS 2025?\"\n",
    "    ]\n",
    "    return [(base_context, random.choice(questions)) for _ in range(n)]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(pairs: List[Tuple[str, str]]) -> float:\n",
    "    \"\"\"\n",
    "    Executa infer√™ncia uma por uma na CPU.\n",
    "    N√£o h√° paralelismo aqui: cada par √© processado individualmente.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for context, question in pairs:\n",
    "        inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        _ = model(**inputs)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(pairs: List[Tuple[str, str]]) -> float | None:\n",
    "    \"\"\"\n",
    "    Executa infer√™ncia de todos os pares em um √∫nico batch na GPU.\n",
    "    A tokeniza√ß√£o e a execu√ß√£o s√£o paraleliz√°veis e otimizadas pela GPU.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "        questions, contexts = zip(*pairs)\n",
    "        inputs = tokenizer(\n",
    "            list(questions), list(contexts),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True, truncation=True\n",
    "        ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # sincroniza antes da medi√ß√£o\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model(**inputs)\n",
    "        torch.cuda.synchronize()  # sincroniza ap√≥s execu√ß√£o\n",
    "        return time.perf_counter() - t0\n",
    "\n",
    "    finally:\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Executa o benchmark variando o n√∫mero de pares QA.\n",
    "    Para cada N, registra tempos absolutos e ganho de speed-up.\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        qa_pairs = make_qa_pairs(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(qa_pairs)\n",
    "        t_gpu = run_gpu_batch(qa_pairs)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b2aeec-f4d5-45d8-9af3-d357aa60725b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#6 - Reconhecimento de Entidades Nomeadas com BERTimbau-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25a6918-d08c-4052-a0c0-c6379a099f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio demonstra como realizar Reconhecimento de Entidades Nomeadas (NER) em portugu√™s brasileiro, utilizando o modelo marcosgg/bert-base-pt-ner-enamex, baseado no BERTimbau. O modelo √© aplicado sobre v√°rias senten√ßas de forma paralela, usando infer√™ncia em lote na GPU, e o tempo de execu√ß√£o √© medido com precis√£o.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "O NER √© amplamente utilizado em:\n",
    "- Extra√ß√£o de nomes de pessoas, organiza√ß√µes, locais em textos jornal√≠sticos e cient√≠ficos\n",
    "- Indexa√ß√£o sem√¢ntica em mecanismos de busca\n",
    "- Automatiza√ß√£o de an√°lise jur√≠dica e contratos\n",
    "- An√°lise de curr√≠culos e documentos administrativos\n",
    "- Pr√©-processamento em sistemas de recomenda√ß√£o e chatbots\n",
    "\n",
    "üîÅ Input e Output\n",
    "- Input: Lista de senten√ßas em portugu√™s contendo poss√≠veis entidades nomeadas.\n",
    "- Output: Para cada senten√ßa, retorna as entidades encontradas, seu tipo (ex: PER, LOC, ORG) e a confian√ßa da predi√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bb8c38-0155-4594-b6ad-d63cbaf3f793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TokenClassificationPipeline\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO DO MODELO E EXEMPLOS DE TESTE\n",
    "# ==============================================================================\n",
    "MODEL_ID  = \"marcosgg/bert-base-pt-ner-enamex\"  # Modelo treinado para portugu√™s (ENAMEX)\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "\n",
    "SENTENCES = [\n",
    "    \"A XXV ERAD/RS 2025 ocorrer√° em Foz do Igua√ßu, no Paran√°, entre 23 e 25 de abril.\",\n",
    "    \"Os participantes visitar√£o o Itaipu Parquetec durante o evento.\",\n",
    "    \"Haver√° minicursos sobre exaescala com instrutores da Universidade Federal do Rio Grande do Sul.\",\n",
    "    \"O hackathon de otimiza√ß√£o de kernels CUDA contar√° com apoio da NVIDIA.\",\n",
    "    \"Trabalhos aceitos no F√≥rum de Inicia√ß√£o Cient√≠fica ser√£o publicados pela SBC.\"\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO: CARREGAMENTO DO MODELO NA GPU\n",
    "# ==============================================================================\n",
    "def load_pipeline(model_id: str) -> TokenClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Carrega o modelo e tokenizer, e prepara uma pipeline para classifica√ß√£o de tokens.\n",
    "    Atribui√ß√£o expl√≠cita √† GPU com dtype otimizado (FP16) para economizar mem√≥ria da T4.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ Esta vers√£o requer uma GPU CUDA; nenhuma detectada.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        torch_dtype=torch.float16  # Reduz uso de VRAM mantendo performance\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    return TokenClassificationPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0,  # 0 corresponde √† GPU atual\n",
    "        aggregation_strategy=\"simple\"  # Agrupa tokens cont√≠guos da mesma entidade\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO: EXECU√á√ÉO DO RECONHECIMENTO EM LOTE\n",
    "# ==============================================================================\n",
    "@torch.inference_mode()\n",
    "def ner_batch(sentences: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Executa a infer√™ncia do modelo NER para um lote de frases, usando GPU.\n",
    "    A fun√ß√£o mede o tempo total de execu√ß√£o e imprime entidades detectadas.\n",
    "    \"\"\"\n",
    "    pipe = load_pipeline(MODEL_ID)\n",
    "\n",
    "    # Sincroniza√ß√£o de CUDA antes da medi√ß√£o para eliminar opera√ß√µes pendentes\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Aqui ocorre o paralelismo: todas as frases s√£o processadas em lote\n",
    "    results = pipe(sentences, batch_size=len(sentences), truncation=True)\n",
    "\n",
    "    # Sincroniza√ß√£o ap√≥s execu√ß√£o para medir corretamente o tempo de infer√™ncia\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    print(f\"\\n=== BERTimbau NER ‚Ä¢ lote √∫nico (GPU) ‚Äî {elapsed:.2f} s ===\\n\")\n",
    "    for sent, ents in zip(sentences, results):\n",
    "        print(f\"üîπ Senten√ßa: {sent}\")\n",
    "        for ent in ents:\n",
    "            print(f\"   ‚Ü≥ {ent['word']}  |  {ent['entity_group']}  |  score {ent['score']:.2f}\")\n",
    "        print()\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    ner_batch(SENTENCES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498ceb29-2146-4ba6-9c4e-f04a1478deb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from typing import List, Dict\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO DO MODELO\n",
    "# ==============================================================================\n",
    "MODEL_ID   = \"marcosgg/bert-base-pt-ner-enamex\"\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "SIZES      = [2**i for i in range(9)]  # N√∫mero de senten√ßas: 1 a 256\n",
    "SEED       = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO DE CARREGAMENTO DO MODELO\n",
    "# ==============================================================================\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# ==============================================================================\n",
    "# GERA√á√ÉO DE SENTEN√áAS EXEMPLO\n",
    "# ==============================================================================\n",
    "def make_sentences(n: int) -> List[str]:\n",
    "    \"\"\"Gera senten√ßas contendo entidades nomeadas (NER).\"\"\"\n",
    "    templates = [\n",
    "    \"A XXV ERAD/RS 2025 ocorrer√° em Foz do Igua√ßu, no Paran√°, entre 23 e 25 de abril.\",\n",
    "    \"Os participantes visitar√£o o Itaipu Parquetec durante o evento.\",\n",
    "    \"Haver√° minicursos sobre exaescala com instrutores da Universidade Federal do Rio Grande do Sul.\",\n",
    "    \"O hackathon de otimiza√ß√£o de kernels CUDA contar√° com apoio da NVIDIA.\",\n",
    "    \"Trabalhos aceitos no F√≥rum de Inicia√ß√£o Cient√≠fica ser√£o publicados pela SBC.\"\n",
    "    ]\n",
    "    return [random.choice(templates) + f\" Example {i}\" for i in range(n)]\n",
    "\n",
    "# ==============================================================================\n",
    "# INFER√äNCIA SEQUENCIAL NA CPU\n",
    "# ==============================================================================\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(sentences: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Executa NER uma frase por vez usando CPU. Simula casos onde n√£o h√° paralelismo.\n",
    "    Ideal para entender a lat√™ncia por amostra em ambientes de baixa capacidade.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        _ = model(**inputs)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ==============================================================================\n",
    "# INFER√äNCIA EM LOTE NA GPU\n",
    "# ==============================================================================\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(sentences: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Executa NER sobre todas as frases de uma s√≥ vez (paralelismo em batch).\n",
    "    √â aqui que a GPU brilha: processando v√°rias entradas em paralelo com grande throughput.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\", non_blocking=True)\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model(**inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        return time.perf_counter() - t0\n",
    "    finally:\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO DE BENCHMARK COMPARATIVO\n",
    "# ==============================================================================\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Executa benchmark comparando os tempos de execu√ß√£o entre CPU e GPU\n",
    "    para diferentes quantidades de senten√ßas.\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        sentences = make_sentences(n)\n",
    "\n",
    "        t_cpu = run_cpu_seq(sentences)\n",
    "        t_gpu = run_gpu_batch(sentences)\n",
    "\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dbfeee-d6db-4fc6-b8d3-ae82ffa93a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#7 - Fine Tuning de BERTimbau para Classifica√ß√£o Bin√°ria de Sentimentos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f558366-def0-4cb8-aa6c-83728fbffb85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio apresenta o processo completo de fine-tuning (ajuste fino) de um modelo de linguagem pr√©-treinado, o BERTimbau, para uma tarefa supervisionada de classifica√ß√£o bin√°ria de sentimentos. O treinamento √© executado em lote na GPU, utilizando o Trainer da Hugging Face. A base de dados √© sint√©tica, com frases positivas e negativas simulando avalia√ß√µes sobre o evento ERAD/RS 2025.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Esse tipo de ajuste fino √© usado em:\n",
    "- Classificadores de sentimento para redes sociais, SAC, e-commerce etc.\n",
    "- Customiza√ß√£o de modelos gen√©ricos para dom√≠nios espec√≠ficos (jur√≠dico, educacional, t√©cnico).\n",
    "- Aprimoramento de chatbots com respostas emocionais mais contextuais.\n",
    "- Treinamento supervisionado de classificadores especializados usando poucas amostras.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "- Input: Dataset sint√©tico contendo frases com sentimentos positivos e negativos (em portugu√™s), com r√≥tulo 1 para positivo e 0 para negativo.\n",
    "- Output: Modelo ajustado para distinguir o sentimento das frases. A sa√≠da principal deste script √© o tempo de treinamento com batch e GPU, √∫til para benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296b3463-ccc5-4644-bc63-f529fdba2830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "# ==============================================================================\n",
    "from __future__ import annotations\n",
    "import os, time, random, torch, tempfile, shutil\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO DO MODELO E AMBIENTE\n",
    "# ==============================================================================\n",
    "MODEL_ID  = \"neuralmind/bert-base-portuguese-cased\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "SEED      = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# GERA√á√ÉO DO DATASET SINT√âTICO BALANCEADO\n",
    "# ==============================================================================\n",
    "def gerar_dataset(n: int = 600) -> Dataset:\n",
    "    \"\"\"Cria um dataset balanceado de frases positivas e negativas sobre a ERAD/RS.\"\"\"\n",
    "    pos_base = [\n",
    "        \"Estou ansioso para participar da ERAD/RS 2025 em Foz do Igua√ßu!\",\n",
    "        \"A visita t√©cnica √† Itaipu Parquetec ser√° fant√°stica.\",\n",
    "        \"Os minicursos de exaescala parecem imperd√≠veis.\",\n",
    "        \"Adorei a prorroga√ß√£o do prazo, terei tempo de submeter meu artigo.\",\n",
    "        \"O hackathon com GPUs NVIDIA vai ser incr√≠vel.\"\n",
    "    ]\n",
    "    neg_base = [\n",
    "        \"Achei a taxa de inscri√ß√£o da ERAD/RS muito alta.\",\n",
    "        \"Fiquei frustrado com o atraso na divulga√ß√£o dos tutoriais.\",\n",
    "        \"N√£o gostei que o evento n√£o oferece op√ß√µes de hospedagem acess√≠veis.\",\n",
    "        \"A mudan√ßa de cronograma me atrapalhou bastante.\",\n",
    "        \"Achei a p√°gina de submiss√£o confusa.\"\n",
    "    ]\n",
    "    textos, labels = [], []\n",
    "    for i in range(n):\n",
    "        if i % 2 == 0:\n",
    "            textos.append(random.choice(pos_base) + f\" (exemplo {i})\")\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            textos.append(random.choice(neg_base) + f\" (exemplo {i})\")\n",
    "            labels.append(0)\n",
    "    return Dataset.from_dict({\"text\": textos, \"label\": labels})\n",
    "\n",
    "# ==============================================================================\n",
    "# TOKENIZA√á√ÉO COM PADDING/TRUNCATION\n",
    "# ==============================================================================\n",
    "def tokenize(ds: Dataset, tok) -> Dataset:\n",
    "    return ds.map(\n",
    "        lambda x: tok(x[\"text\"], truncation=True, padding=\"max_length\"),\n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# FINE-TUNING NA GPU ‚Äî ATIVA√á√ÉO DE PARALELISMO\n",
    "# ==============================================================================\n",
    "@torch.inference_mode(False)\n",
    "def treinar_gpu(dataset: Dataset) -> float:\n",
    "    \"\"\"\n",
    "    Realiza treinamento em GPU com suporte a FP16.\n",
    "    Importante: toda a carga do modelo e dados s√£o transferidos para CUDA,\n",
    "    ativando paralelismo massivo no treinamento com batch size 16.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ Nenhuma GPU CUDA detectada ‚Äì esta vers√£o exige GPU.\")\n",
    "\n",
    "    # Carrega tokenizer e transforma o dataset para o formato compat√≠vel\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "    ds_tok = tokenize(dataset, tok).remove_columns([\"text\"])\n",
    "    ds_tok.set_format(\"torch\")\n",
    "\n",
    "    # Carrega o modelo em formato FP16 direto na GPU\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        num_labels=2,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Cria diret√≥rio tempor√°rio para n√£o salvar nada em disco\n",
    "    workdir = tempfile.mkdtemp(prefix=\"eradrs_finetune_\")\n",
    "\n",
    "    # Configura√ß√µes do HuggingFace Trainer\n",
    "    args = TrainingArguments(\n",
    "        output_dir=workdir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=20,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"no\"  # n√£o salva checkpoints\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds_tok)\n",
    "\n",
    "    # In√≠cio da contagem com sincroniza√ß√£o CUDA (para medir com precis√£o)\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    trainer.train()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    shutil.rmtree(workdir, ignore_errors=True)\n",
    "    return elapsed\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    ds = gerar_dataset(n=600)\n",
    "    print(\"\\nFine-tuning BERTimbau (1 √©poca, 600 exemplos)\")\n",
    "    print(f\"M√°quina: g4dn.8xlarge (T4 16 GB)  ¬∑  Batch 16  ¬∑  FP16\\n\")\n",
    "    tempo = treinar_gpu(ds)\n",
    "    print(f\"Tempo total de treinamento na GPU: {tempo:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cd0597-ad1f-4eb7-ae6f-034acae20028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "from typing import List\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO GLOBAL\n",
    "# ==============================================================================\n",
    "MODEL_ID = \"neuralmind/bert-base-portuguese-cased\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# GERA√á√ÉO DO DATASET SINT√âTICO\n",
    "# ==============================================================================\n",
    "def generate_data(n: int = 1000) -> Dataset:\n",
    "    \"\"\"\n",
    "    Cria um dataset de sentimentos simulados com exemplos balanceados.\n",
    "    0 = negativo, 1 = positivo.\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        f\"Estou ansioso para participar da ERAD/RS 2025 em Foz do Igua√ßu!, exemplo {i}\" if i % 2 == 0\n",
    "        else f\"Achei a p√°gina de submiss√£o confusa., exemplo {i}\"\n",
    "        for i in range(n)\n",
    "    ]\n",
    "    labels = [1 if i % 2 == 0 else 0 for i in range(n)]\n",
    "    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# ==============================================================================\n",
    "# TOKENIZA√á√ÉO\n",
    "# ==============================================================================\n",
    "def tokenize_dataset(ds: Dataset, tokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Transforma textos em tensores usando o tokenizer DistilBERT.\n",
    "    Inclui truncamento e padding fixo (necess√°rio para batching).\n",
    "    \"\"\"\n",
    "    return ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# FASE DE TREINAMENTO (CPU OU GPU)\n",
    "# ==============================================================================\n",
    "def train_model(dataset: Dataset, device: str = \"cpu\") -> float:\n",
    "    \"\"\"\n",
    "    Realiza treinamento por 1 √©poca em CPU ou GPU.\n",
    "    Retorna o tempo total (em segundos).\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "    tokenized = tokenize_dataset(dataset, tokenizer).remove_columns([\"text\"])\n",
    "    tokenized.set_format(\"torch\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID, cache_dir=CACHE_DIR, num_labels=2, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    output_dir = f\"/tmp/tmp_finetune_{device}\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"no\",\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=50,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"no\",\n",
    "        save_steps=0,\n",
    "        no_cuda=(device == \"cpu\")  # importante: ativa ou desativa GPU\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized,\n",
    "    )\n",
    "\n",
    "    # In√≠cio da medi√ß√£o (com sincroniza√ß√£o se for GPU)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    trainer.train()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# ==============================================================================\n",
    "# BENCHMARK COMPARATIVO\n",
    "# ==============================================================================\n",
    "def benchmark():\n",
    "    dataset = generate_data(n=500)  # dataset leve para demonstra√ß√£o r√°pida\n",
    "    print(\"\\nBenchmark: Fine-tuning DistilBERT for 1 epoch on 500 examples\")\n",
    "    print(f\"{'Device':>6} | {'Time (s)':>9}\")\n",
    "    print(\"-\" * 21)\n",
    "\n",
    "    # Execu√ß√£o sequencial em CPU (sem paralelismo massivo)\n",
    "    t_cpu = train_model(dataset, device=\"cpu\")\n",
    "    print(f\"{'CPU':>6} | {t_cpu:9.2f}\")\n",
    "\n",
    "    # Execu√ß√£o paralela na GPU (ativando CUDA e uso massivo de tensores FP32)\n",
    "    if torch.cuda.is_available():\n",
    "        t_gpu = train_model(dataset, device=\"cuda\")\n",
    "        print(f\"{'GPU':>6} | {t_gpu:9.2f}\")\n",
    "        print(f\"\\nSpeed-up: {t_cpu / t_gpu:.2f}√ó\")\n",
    "    else:\n",
    "        print(f\"{'GPU':>6} | {'  N/A':>9}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8dfe2d9-0e27-41de-99f8-3aca71a60e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#8 - Preenchimento de M√°scara com BERTimbau (Fill-Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2732db-0356-4cc9-bcbc-a580c88d147f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üìå Resumo do Exerc√≠cio\n",
    "\n",
    "Este exerc√≠cio demonstra como aplicar o modelo BERTimbau para a tarefa de Masked Language Modeling (MLM) ‚Äî isto √©, prever palavras ausentes em frases contendo o token [MASK]. A execu√ß√£o ocorre de forma paralela em lote na GPU, e o tempo de infer√™ncia √© medido para diferentes volumes de entrada (de 32 a 2048 frases). O objetivo principal √© evidenciar o impacto positivo do paralelismo vetorizado com CUDA em tarefas de infer√™ncia textual com BERT.\n",
    "\n",
    "üí° Aplica√ß√£o\n",
    "\n",
    "Modelos MLM s√£o usados em:\n",
    "- Pr√©-treinamento de modelos BERT-like;\n",
    "- Ferramentas de autocorre√ß√£o ou sugest√£o de palavras;\n",
    "- Recupera√ß√£o de informa√ß√£o com base em contexto parcial;\n",
    "- Aprendizado contextual de palavras em tarefas downstream;\n",
    "- Gera√ß√£o de datasets com data augmentation por substitui√ß√£o de tokens.\n",
    "\n",
    "üîÅ Input e Output\n",
    "\n",
    "- Input: Lista de frases em portugu√™s com um token [MASK] por senten√ßa, como: \"O hackathon usar√° GPUs [MASK]. Amostra 42.\"\n",
    "- Output: O modelo realiza uma passagem direta (forward pass), gerando logits para prever qual palavra deve ocupar o lugar do token [MASK]. Neste benchmark, o output analisado √© o tempo de execu√ß√£o, e n√£o a predi√ß√£o textual em si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dff18a5-28b9-4d7d-a541-ec885065d978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, random, torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import List\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "MODEL_ID   = \"neuralmind/bert-base-portuguese-cased\"  # üáßüá∑ BERTimbau-Base\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "SIZES      = [2**i for i in range(5, 12)]   # de 32 at√© 2048 senten√ßas\n",
    "SEED       = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ==============================================================================\n",
    "# CARREGAMENTO DO MODELO + TOKENIZER (em FP16 direto na GPU)\n",
    "# ==============================================================================\n",
    "def load_model(model_id: str):\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"üö´ Esta vers√£o √© apenas-GPU ‚Äî nenhuma CUDA detectada.\")\n",
    "\n",
    "    # Tokenizer carrega vocabul√°rio e regras de segmenta√ß√£o\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, cache_dir=CACHE_DIR)\n",
    "\n",
    "    # Modelo de linguagem mascarada (BERT) carregado em float16 para otimizar VRAM\n",
    "    mdl = AutoModelForMaskedLM.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")  # envia tudo para a GPU\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID)\n",
    "model.eval()  # importante: desativa dropout, ativa modo de infer√™ncia\n",
    "\n",
    "# ==============================================================================\n",
    "# GERA√á√ÉO DE SENTEN√áAS COM M√ÅSCARA (exemplos baseados na ERAD/RS)\n",
    "# ==============================================================================\n",
    "def make_masked_sentences(n: int) -> List[str]:\n",
    "    \"\"\"Cria *n* frases com token [MASK] para infer√™ncia.\"\"\"\n",
    "    templates = [\n",
    "        f\"A ERAD/RS 2025 acontecer√° em {MASK_TOKEN}, no Paran√°. Exemplo {{}}.\",\n",
    "        f\"Os minicursos abordar√£o computa√ß√£o de {MASK_TOKEN} escala. Caso {{}}.\",\n",
    "        f\"O hackathon de otimiza√ß√£o usar√° GPUs {MASK_TOKEN}. Amostra {{}}.\",\n",
    "        f\"A visita t√©cnica ocorrer√° no {MASK_TOKEN} Parquetec. Variante {{}}.\",\n",
    "        f\"O prazo de submiss√£o foi {MASK_TOKEN} pela organiza√ß√£o. Item {{}}.\"\n",
    "    ]\n",
    "    return [random.choice(templates).format(i) for i in range(n)]\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO EM LOTE NA GPU\n",
    "# ==============================================================================\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(sentences: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Realiza infer√™ncia do tipo fill-mask sobre todas as senten√ßas em lote na GPU.\n",
    "    Retorna o tempo total decorrido.\n",
    "    \"\"\"\n",
    "    # Pr√©-processamento: tokeniza√ß√£o + tensoriza√ß√£o ‚Üí CUDA\n",
    "    inputs = tokenizer(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "    # Medi√ß√£o sincronizada do tempo de execu√ß√£o na GPU\n",
    "    torch.cuda.synchronize(); t0 = time.perf_counter()\n",
    "    _ = model(**inputs)  # forward pass com tudo vetorizado na T4\n",
    "    torch.cuda.synchronize(); t1 = time.perf_counter()\n",
    "\n",
    "    return t1 - t0\n",
    "\n",
    "# ==============================================================================\n",
    "# BENCHMARK COMPARATIVO: ESCALABILIDADE COM LOTE\n",
    "# ==============================================================================\n",
    "def benchmark():\n",
    "    print(\"\\nBenchmark Fill-Mask ‚Äì BERTimbau (GPU, T4 16 GB)\")\n",
    "    print(f\"{'N':>6} | {'GPU (s)':>8}\")\n",
    "    print(\"-\" * 18)\n",
    "\n",
    "    for n in SIZES:\n",
    "        masked = make_masked_sentences(n)\n",
    "        t_gpu  = run_gpu_batch(masked)\n",
    "        print(f\"{n:6d} | {t_gpu:8.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e9d948-e61e-4168-a9d7-79c2fa6b44bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import List, Dict\n",
    "\n",
    "# --------------------------- Configura√ß√£o ---------------------------\n",
    "MODEL_ID   = \"neuralmind/bert-base-portuguese-cased\"\n",
    "CACHE_DIR  = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "MASK_TOKEN = \"[MASK]\"\n",
    "SIZES      = [2**i for i in range(5, 12)]  # de 32 at√© 2048 senten√ßas\n",
    "SEED       = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# --------------------------- Carregamento ---------------------------\n",
    "def load_model(model_id: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_id, cache_dir=cache_dir, ignore_mismatched_sizes=True)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(MODEL_ID, CACHE_DIR)\n",
    "model.eval()  # modo infer√™ncia (desativa dropout etc.)\n",
    "\n",
    "# --------------------------- Dados Sint√©ticos -----------------------\n",
    "def make_masked_sentences(n: int) -> List[str]:\n",
    "    \"\"\"Gera *n* frases distintas com token [MASK].\"\"\"\n",
    "    templates = [\n",
    "        f\"A ERAD/RS 2025 acontecer√° em {MASK_TOKEN}, no Paran√°. Exemplo {{}}.\",\n",
    "        f\"Os minicursos abordar√£o computa√ß√£o de {MASK_TOKEN} escala. Caso {{}}.\",\n",
    "        f\"O hackathon de otimiza√ß√£o usar√° GPUs {MASK_TOKEN}. Amostra {{}}.\",\n",
    "        f\"A visita t√©cnica ocorrer√° no {MASK_TOKEN} Parquetec. Variante {{}}.\",\n",
    "        f\"O prazo de submiss√£o foi {MASK_TOKEN} pela organiza√ß√£o. Item {{}}.\"\n",
    "    ]\n",
    "    return [random.choice(templates).format(i) for i in range(n)]\n",
    "\n",
    "# --------------------------- Infer√™ncia na CPU ----------------------\n",
    "@torch.inference_mode()\n",
    "def run_cpu_seq(sentences: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Executa infer√™ncia de MLM sequencialmente na CPU.\n",
    "    Cada senten√ßa √© processada isoladamente.\n",
    "    \"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    t0 = time.perf_counter()\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", padding=True, truncation=True).to(\"cpu\")\n",
    "        _ = model(**inputs)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "# --------------------------- Infer√™ncia na GPU ----------------------\n",
    "@torch.inference_mode()\n",
    "def run_gpu_batch(sentences: List[str]) -> float | None:\n",
    "    \"\"\"\n",
    "    Executa infer√™ncia MLM em lote na GPU.\n",
    "    Todas as senten√ßas s√£o vetorizadas e processadas em paralelo.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    try:\n",
    "        model.to(\"cuda\", non_blocking=True)\n",
    "        inputs = tokenizer(\n",
    "            sentences,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(\"cuda\", non_blocking=True)\n",
    "\n",
    "        torch.cuda.synchronize(); t0 = time.perf_counter()\n",
    "        _ = model(**inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        return time.perf_counter() - t0\n",
    "    finally:\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------------- Benchmark Comparativo ------------------\n",
    "def benchmark() -> Dict[int, Dict[str, float | None]]:\n",
    "    \"\"\"\n",
    "    Executa benchmark variando o n√∫mero de frases com [MASK].\n",
    "    Compara tempo entre execu√ß√£o sequencial na CPU e execu√ß√£o em lote na GPU.\n",
    "    \"\"\"\n",
    "    results: Dict[int, Dict[str, float | None]] = {}\n",
    "    print(f\"\\n{'N':>5} | {'CPU-SEQ (s)':>11} | {'GPU (s)':>8} | Speed-up\")\n",
    "    print(\"-\" * 43)\n",
    "\n",
    "    for n in SIZES:\n",
    "        masked = make_masked_sentences(n)\n",
    "        t_cpu = run_cpu_seq(masked)\n",
    "        t_gpu = run_gpu_batch(masked)\n",
    "        speed = (t_cpu / t_gpu) if (t_gpu and t_gpu > 0) else None\n",
    "\n",
    "        results[n] = {\"cpu\": t_cpu, \"gpu\": t_gpu, \"speedup\": speed}\n",
    "        gpu_txt = f\"{t_gpu:8.2f}\" if t_gpu else \"   N/A  \"\n",
    "        spd_txt = f\"{speed:6.2f}√ó\" if speed else \"  ‚Äî\"\n",
    "        print(f\"{n:5d} | {t_cpu:11.2f} | {gpu_txt} | {spd_txt}\")\n",
    "    return results\n",
    "\n",
    "# --------------------------- Execu√ß√£o Principal ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Minicurso AI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}